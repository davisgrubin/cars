---
title: "Gelman Exercises Chapter 2"
output: "html_document"
---

<h1>Question 1</h1>
1.A test is graded from 0 to 50, with an average score of 35 and a std. deviation of 10. For comparison of tests, it would be convenient to rescale to a mean of 100 and standard deviation of 15.

<h2>a)</h2>
How can the scores be linearly transformed to have this new mean and standard deviation?

Changing the mean of the scores only requires addition (shifting the distribution to the right), and doesn't require a change in scaling, while changing the variance/std deviation will require multiplication that will effect the mean. Because of this, I've decided to scale the scores to adjust the std.deviation first and to then adjust the mean of the distribution as needed.   
    
    Effect on variance of linear transformation:
   $$Y = aX + b \\ \sigma^{2} = (x_{i} - \mu_{x})^{2}/n \\ (ax_{i} - (ax_{i}/n))^{2}/n \\ (ax_{i} - a\mu_{x})^{2}/n \\ a^{2}x_{i}^{2} - a^{2}\mu_{x}^{2}/n \\ a^{2}(x_{i} - \mu_{x})^{2}/n \\ a^2\sigma^2  $$ 
    Solving for linear transformation to desired variance:
    $$\sigma = 10 \\ \sigma^{2} = 100 \\ a^{2}100 = 225 \\ a=1.5$$
    Now plugging in for transformation for desired mean:
    $$a\mu + b = 100 \\ 1.5*35 + b = 100 \\ b = 47.5  $$
    Checking in R to see if these are correct:

```{r,message=FALSE,warning=FALSE}
library(distr)
test_scores <- Norm(mean = 35, sd = 10)
1.5 * test_scores + 47.5
```
    
<h2>b)</h2>
There is another linear transformation that also rescales the scores to have mean 100 and std. deviation 15. What is it, and why would you *not* want to use it for this purpose?

Since standard deviation is an absolute measure, we could make $a$ = -1.5 , and adjust $b$ = 152.5 to achieve the desired $\mu$ and $\sigma$ as seen below.
```{r, warning=FALSE}
-1.5 * test_scores + 152.5
```
However, this is undesirable because by multiplying the positive scores by a negative number, scores in the original group that were lower than others will be closer to zero, and after the transformation will be higher than a score that was originally higher before the transformation. 
```{r}
should_be_lower = -1.5*(30) + 152.5
should_be_higher = -1.5*(35) + 152.5
msg1 <- paste(c("was 30 before transformation: after transformation is"), c(should_be_lower))
msg2 <- paste(c("was 35 before transformation: after transformation is"),c(should_be_higher))
cat(paste0(msg1,"\n",msg2))
```

<h1>Question 2</h1>

The following are the proportions of girl births in Vienna for each month in 1908 and 1909 (out of an average of 3900 births per month.)

```{r}
births <- c(.4777,.4875,.4859,.4754,.4874,.4864,.4813,.4787,.4895,.4797,.4876,.4859,.4857,.4907,.5010,.4903,.4860,.4911,.4871,.4725,.4822,.4870,.4823,.4973)
```
von Mises used these proportions to claim that the sex ratios were less variable than would be expected by chance.

<h2>a)</h2>

Compute the std. deviation of these proportions and compare to the std. deviation that would be expected if the sexes of babies were independently decided with a constant probability over the 24-month period.

```{r}
sd(births)
```
```{r}
#self-defined to check for understanding
sqrt(sum((mean(births) - births)^2)/(length(births) - 1))
```
This is a binomial probability model since there are only 2 possible outcomes - Male or Female. This means we can use the formula for calculating the std. deviation of a binomial distribution of proportions. 

$$\sqrt{p * (1-p)/n}\\
n = 24 \\p= 0.485675$$
Calculating the std. deviation with the above formula we get the following:

```{r}
births_prop_sd <- sqrt((mean(births) * (1-mean(births))) / 3900)
print(births_prop_sd)
```
From this we can see that the observed std. deviation of ~.006 is less than the expected standard deviation of ~.008 from a random binomial distribution. 

<h2>b)</h2>

The actual and theoretical standard deviations from (a) differ, of course. Is this difference statistically significant? (Hint: under the randomness model, the actual variance should have a distribution with expected value equal to the theoretical variance, and proportional to a $\chi^2$ with 23 degrees of freedom)

```{r}
scale_factor <- births_prop_sd^2/23 
sqrt(scale_factor * q(Chisq(df=23))(c(.025,.975)))
```

The above calculations take the confidence interval at the 5% significance level for a $\chi^2$ distribution with 23 degrees of freedom, then scales them down proportionally by a factor of $d.o.f/\sigma^2$, following the hint in the question. Since the ~.006 observed std. deviation occurs within this confidence interval we can conclude that observed and theoretical standard deviations' difference is not statistically significant.

<h1>Question 3</h1>

Demonstration of the Central Limit Theorem: let $x = x_{1} + ... +x_{20}$, the sum of 20 independent Uniform(0,1) random variables. In R, create 1000 simulations of $x$ and plot their histogram. On the histogram, overlay a graph of the normal density function. Comment on any differences between the histogram and the curve. 

```{r,message=FALSE,warning=FALSE}
library(ggplot2)
sums_of_rvs <- replicate(1000,sum(r(Unif(0,1))(20)))
df <- data.frame(x = sums_of_rvs )
ggplot(df, aes(x = x)) + 
	geom_histogram(aes(y = ..density..,binwidth=10)) + 
	stat_function(
		fun = dnorm, 
		args = with(df, c(mean = mean(x), sd = sd(x)))
	)
```

There don't appear to be any particularly startling differences between the histogram and the normal curve, other than a small jump slightly above 10 that seems to be unexpectedly high above the curve (though I'm not going to spend the time to see if this is just an artifact of binning).









